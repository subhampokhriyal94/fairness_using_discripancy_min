{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Support Vector Machine in just a few Lines of Python Code\n",
    "\n",
    "*Content created by [webstudio Richter](http://bestinnovations.de/wordpress/a-perceptron-in-just-a-few-lines-of-python-code/) alias Mavicc on March 30. 2017.*\n",
    "\n",
    "In the last tutorial we coded a perceptron using stochastic gradient descent. The perceptron solved a linear seperable classification problem, by finding a hyperplane seperating the two classes.\n",
    "\n",
    "The point here is, that the perceptron just finds one possible hyperplane, not necessarily the optimal hyperplane. That can lead to lower generalization performance.\n",
    "\n",
    "Here support vector machines (svm) come into place. In contrast to perceptrons, svms try to find a hyperplane, which maximizes the margin between two classes.\n",
    "\n",
    "As last time, we will focus on developing the code, not so much the theorety of svms.\n",
    "\n",
    "I highly advise you the book of Schölkopf & Smola. Do not let the math scrare you, as they explain the basics of machine learning in a really comprehensive way:\n",
    "\n",
    "**Schölkopf & Smola** (2002). Learning with Kernels. Support Vector Machines, Regularization, Optimization, and Beyond.\n",
    "\n",
    "Furthermore there is a short introduction to the mathematical backgrounds of svm in the following MIT paper:\n",
    "\n",
    "https://www.google.de/search?client=ubuntu&channel=fs&q=svm+standford&ie=utf-8&oe=utf-8&gfe_rd=cr&ei=UfDhWM_BMunVXredsIgI#q=support+vector+machine+tutorial&channel=fs&start=10&*\n",
    "\n",
    "## Give me the Code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.58876117   3.17458055  11.11863105]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([\n",
    "    [-2,4,-1],\n",
    "    [4,1,-1],\n",
    "    [1, 6, -1],\n",
    "    [2, 4, -1],\n",
    "    [6, 2, -1],\n",
    "\n",
    "])\n",
    "\n",
    "y = np.array([-1,-1,1,1,1])\n",
    "\n",
    "def svm_sgd(X, Y):\n",
    "\n",
    "    w = np.zeros(len(X[0]))\n",
    "    eta = 1\n",
    "    epochs = 100000\n",
    "\n",
    "\n",
    "    for epoch in range(1,epochs):\n",
    "        for i, x in enumerate(X):\n",
    "            if (Y[i]*np.dot(X[i], w)) < 1:\n",
    "                w = w + eta * ( (X[i] * Y[i]) + (-2  *(1/epoch)* w) )\n",
    "            else:\n",
    "                w = w + eta * (-2  *(1/epoch)* w)\n",
    "\n",
    "    return w\n",
    "\n",
    "w = svm_sgd(X,y)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Ingredients \n",
    "\n",
    "First we will import numpy to easily manage linear algebra and calculus operations in python. To plot the learning progress later on, we will use matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent\n",
    "\n",
    "As for the perceptron, we use python 3 and numpy. The svm will learn using the stochastic gradient descent algorithm (SGD). Gradient Descent minimizes a function by following the gradients of the cost function. For further details see:\n",
    "\n",
    "Wikipedia - stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the Error \n",
    "\n",
    "To calculate the error of a prediction we first need to define the objective function of the svm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hinge Loss Function\n",
    "\n",
    "To do this, we need to define the loss function, to calculate the prediction error. We will use hinge loss for our perceptron:\n",
    "\n",
    "$$c(x, y, f(x)) = (1 - y * f(x))_+$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$c$ is the loss function, $x$ the sample, $y$ is the true label, $f(x)$ the predicted label.\n",
    "\n",
    "This means the following:\n",
    "$$\n",
    "c(x, y, f(x))= \n",
    "\\begin{cases}\n",
    "    0,& \\text{if } y*f(x)\\geq 1\\\\\n",
    "    1-y*f(x),              & \\text{else}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So consider, if y and f(x) are signed values $(+1,-1)$:\n",
    "\n",
    "<ul>\n",
    "    <li>the loss is 0, if $y*f(x)$ are positive, respective both values have the same sign.</li>\n",
    "    <li>loss is $1-y*f(x)$ if $y*f(x)$ is negative</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objective Function \n",
    "\n",
    "As we defined the loss function, we can now define the objective function for the svm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\underset{w}{min}\\ \\lambda\\parallel w\\parallel^2 + \\ \\sum_{i=1}^n\\big(1-y_i \\langle x_i,w \\rangle\\big)_+$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, our objective of a svm consists of two terms. The first term is a regularizer, the second term the loss. The regularizer balances between margin maximization and loss. To get more informations I advice you the tutorial introduction of the above adviced Schölkopf & Smola book.\n",
    "\n",
    "#### Derive the Objective Function\n",
    "\n",
    "To minimize this function, we need the gradients of this function.\n",
    "\n",
    "As we have two terms, we will derive them seperately using the sum rule in differentiation.\n",
    "\n",
    "$$\n",
    "\\frac{\\delta}{\\delta w_k} \\lambda\\parallel w\\parallel^2 \\ = 2 \\lambda w_k\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\delta}{\\delta w_k} \\big(1-y_i \\langle x_i,w \\rangle\\big)_+ \\ = \\begin{cases}\n",
    "    0,& \\text{if } y_i \\langle x_i,w \\rangle\\geq 1\\\\\n",
    "    -y_ix_{ik},              & \\text{else}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means, if we have a misclassified sample $x_i$, respectively $y_i \\langle x_i,w \\rangle \\ < \\ 1$, we update the weight vector w using the gradients of both terms, if $y_i \\langle x_i,w \\rangle \\geq 1$ we just update w by the gradient of the regularizer. To sum it up, our stochastic gradient descent for the svm looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if $y_i⟨x_i,w⟩ < 1$:\n",
    "$$\n",
    "w = w + \\eta (y_ix_i - 2\\lambda w)\n",
    "$$\n",
    "else:\n",
    "$$\n",
    "w = w + \\eta (-2\\lambda w)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Data Set \n",
    "\n",
    "First we need to define a labeled data set. If you read the perceptron tutorial you will already know it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    [-2, 4],\n",
    "    [4, 1],\n",
    "    [1, 6],\n",
    "    [2, 4],\n",
    "    [6, 2]\n",
    "])\n",
    "\n",
    "y = np.array([-1,-1,1,1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity's sake we again fold the bias term into the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    [-2,4,-1],\n",
    "    [4,1,-1],\n",
    "    [1, 6, -1],\n",
    "    [2, 4, -1],\n",
    "    [6, 2, -1],\n",
    "\n",
    "])\n",
    "\n",
    "y = np.array([-1,-1,1,1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This small toy data set contains two samples labeled with $-1$ and three samples labeled with $+1$. This means we have a binary classification problem, as the data set contains two sample classes. Lets plot the dataset to see, that is is linearly seperable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb5ae0865c0>]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWcAAAEACAYAAABvSbdvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb5ae086630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for d, sample in enumerate(X):\n",
    "    # Plot the negative samples\n",
    "    if d < 2:\n",
    "        plt.scatter(sample[0], sample[1], s=120, marker='_', linewidths=2)\n",
    "    # Plot the positive samples\n",
    "    else:\n",
    "        plt.scatter(sample[0], sample[1], s=120, marker='+', linewidths=2)\n",
    "\n",
    "# Print a possible hyperplane, that is seperating the two classes.\n",
    "plt.plot([-2,6],[6,0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets Start implementing Stochastic Gradient Descent \n",
    "\n",
    "Finally we can code our SGD algorithm using our update rules. In opposite to the perceptrons objective function, we use a regularizer in our algorithm. As we have a small data set, which is easily lineary seperable, this is actually not needed and our stochastic gradient descent algorithm would probably converge faster without it. To give you a more powerfull code at hand, I will keep it in the following algorithm.\n",
    "\n",
    "To keep it simple, we will linearly loop over the sample set. For larger data sets it makes sence, to randomly pick a sample during each iteration in the for-loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def svm_sgd(X, Y):\n",
    "    w = np.zeros(len(X[0]))\n",
    "    eta = 1\n",
    "    epochs = 100000\n",
    "\n",
    "    for epoch in range(1,n):\n",
    "        for i, x in enumerate(X):\n",
    "            if (Y[i]*np.dot(X[i], w)) < 1:\n",
    "                w = w + eta * ( (X[i] * Y[i]) + (-2  *(1/epoch)* w) )\n",
    "            else:\n",
    "                w = w + eta * (-2  *(1/epoch)* w)\n",
    "\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run the sgd $100000$ times. Our learning parameter eta is set to $1$. As a regulizing parameter we choose $1/t$, so this parameter will decrease, as the number of epochs increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code Description Line by Line\n",
    "\n",
    "line <b>2</b>: Initialize the weight vector for the perceptron with zeros<br>\n",
    "line <b>3</b>: Set the learning rate to 1<br>\n",
    "line <b>4</b>: Set the number of epochs<br>\n",
    "line <b>6</b>: Iterate n times over the whole data set. The Iterator is begins with $1$ to avoid division by zero during regularization  parameter calculation<br>\n",
    "line <b>7</b>: Iterate over each sample in the data set. <br>\n",
    "line <b>8</b>: Misclassification condition $y_i \\langle x_i,w \\rangle < 1$<br>\n",
    "line <b>9</b>: Update rule for the weights $w = w + \\eta (y_ix_i - 2\\lambda w)$ including the learning rate $\\eta$ and the regularizer $\\lambda$<br>\n",
    "line <b>11</b>: If classified correctly just update the weight vector by the derived regularizer term $w = w + \\eta (-2\\lambda w)$.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let the SVM learn! \n",
    "\n",
    "Next we can execute our code, to calculate the proper weight vector, which fits out training data. If there are misclassified samples we will print the number of misclassified and correctly classified samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_sgd_plot(X, Y):\n",
    "\n",
    "    w = np.zeros(len(X[0]))\n",
    "    eta = 1\n",
    "    epochs = 100000\n",
    "    errors = []\n",
    "\n",
    "\n",
    "    for epoch in range(1,epochs):\n",
    "        error = 0\n",
    "        for i, x in enumerate(X):\n",
    "            if (Y[i]*np.dot(X[i], w)) < 1:\n",
    "                w = w + eta * ( (X[i] * Y[i]) + (-2  *(1/epoch)* w) )\n",
    "                error = 1\n",
    "            else:\n",
    "                w = w + eta * (-2  *(1/epoch)* w)\n",
    "        errors.append(error)\n",
    "\n",
    "    plt.plot(errors, '|')\n",
    "    plt.ylim(0.5,1.5)\n",
    "    plt.axes().set_yticklabels([])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Misclassified')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEKCAYAAADw2zkCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAf8HfJLfSi9JPfAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb5ae0e0a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "svm_sgd_plot(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above graph shows that the svm makes less misclassifications the more epochs it is running. In contrast to our perceptron we do not reach zero errors permanently, as the svm updates its weight vector by the regularizer, even if the current samples is correctly classified. What looks like a bad deal, is actually the strengh of the SVM, as it always tried to maximize the margin between the two classes by using this regularizer. \n",
    "\n",
    "To sum this up, the perceptron is satisfied, when it finds a seperating hyperplane, our SVM in contrast always tries to optimize the hyperplane, by maximizing the distance between the two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weight vector of the SVM including the bias term after 100000 epochs is $(1.56,  3.17,  11.12)$.<br>\n",
    "We can extract the following prediction function now:\n",
    "\n",
    "\n",
    "$$\n",
    "f(x) = \\langle x,(1.56,3.17)\\rangle - 11.12\n",
    "$$\n",
    "\n",
    "The weight vector is $(1.56,3.17)$ and the bias term is the third entry 11.12."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Lets classify the samples in our data set by hand now, to check if the perceptron learned properly:\n",
    "\n",
    "First sample $(-2, 4)$, supposed to be negative:\n",
    "\n",
    "$$-2*1,56+4*3,17 - 11,12 = sign(-1,56) = -1$$\n",
    "\n",
    "Second sample $(4, 1)$, supposed to be negative:\n",
    "\n",
    "$$4*1,56+1*3,17 - 11,12 = sign(-1,71) = -1$$\n",
    "\n",
    "Third sample $(1, 6)$, supposed to be positive:\n",
    "\n",
    "$$1*1,56+6*3,17-11,12 = sign(9,46) = +1$$\n",
    "\n",
    "Fourth sample $(2, 4)$, supposed to be positive:\n",
    "\n",
    "$$2*1,56+4*3,17 - 11,12 = sign(4,68) = +1$$\n",
    "\n",
    "Fifth sample $(6, 2)$, supposed to be positive:\n",
    "\n",
    "$$6*1,56+2*3,17 - 11,12 = sign(4,58) = +1$$\n",
    "\n",
    "Lets define two test samples now, to check how well our perceptron generalizes to unseen data:\n",
    "\n",
    "First test sample $(2, 2)$, supposed to be negative:\n",
    "\n",
    "$$2*1,56+2*3,17 - 11,12 = sign(-1,66) = -1$$\n",
    "\n",
    "Second test sample $(4, 3)$, supposed to be positive:\n",
    "\n",
    "$$4*1,56+3*3,17 - 11,12 = sign(4,63) = +1$$\n",
    "\n",
    "Both samples are classified right. To check this geometrically, lets plot the samples including test samples and the hyperplane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.quiver.Quiver at 0x7fb5adfda400>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWcAAAEACAYAAABvSbdvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb5adfd30f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for d, sample in enumerate(X):\n",
    "    # Plot the negative samples\n",
    "    if d < 2:\n",
    "        plt.scatter(sample[0], sample[1], s=120, marker='_', linewidths=2)\n",
    "    # Plot the positive samples\n",
    "    else:\n",
    "        plt.scatter(sample[0], sample[1], s=120, marker='+', linewidths=2)\n",
    "\n",
    "# Add our test samples\n",
    "\n",
    "plt.scatter(2,2, s=120, marker='_', linewidths=2, color='yellow')\n",
    "plt.scatter(4,3, s=120, marker='+', linewidths=2, color='blue')\n",
    "\n",
    "# Print the hyperplane calculated by svm_sgd()\n",
    "x2=[w[0],w[1],-w[1],w[0]]\n",
    "x3=[w[0],w[1],w[1],-w[0]]\n",
    "\n",
    "x2x3 =np.array([x2,x3])\n",
    "X,Y,U,V = zip(*x2x3)\n",
    "ax = plt.gca()\n",
    "ax.quiver(X,Y,U,V,scale=1, color='blue')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cvxopt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c13a7f21e356>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcvxopt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcvxopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolvers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cvxopt'"
     ]
    }
   ],
   "source": [
    "# Mathieu Blondel, September 2010\n",
    "# License: BSD 3 clause\n",
    "# http://www.mblondel.org/journal/2010/09/19/support-vector-machines-in-python/\n",
    "\n",
    "# visualizing what translating to another dimension does\n",
    "# and bringing back to 2D:\n",
    "# https://www.youtube.com/watch?v=3liCbRZPrZA\n",
    "\n",
    "# Docs: http://cvxopt.org/userguide/coneprog.html#quadratic-programming\n",
    "# Docs qp example: http://cvxopt.org/examples/tutorial/qp.html\n",
    "\n",
    "# Nice tutorial:\n",
    "# https://courses.csail.mit.edu/6.867/wiki/images/a/a7/Qp-cvxopt.pdf\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from numpy import linalg\n",
    "import cvxopt\n",
    "import cvxopt.solvers\n",
    "             \n",
    "def linear_kernel(x1, x2):\n",
    "    return np.dot(x1, x2)\n",
    "\n",
    "def polynomial_kernel(x, y, p=3):\n",
    "    return (1 + np.dot(x, y)) ** p\n",
    "\n",
    "def gaussian_kernel(x, y, sigma=5.0):\n",
    "    return np.exp(-linalg.norm(x-y)**2 / (2 * (sigma ** 2)))\n",
    "\n",
    "class SVM(object):\n",
    "\n",
    "    def __init__(self, kernel=linear_kernel, C=None):\n",
    "        self.kernel = kernel\n",
    "        self.C = C\n",
    "        if self.C is not None: self.C = float(self.C)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Gram matrix\n",
    "        K = np.zeros((n_samples, n_samples))\n",
    "        for i in range(n_samples):\n",
    "            for j in range(n_samples):\n",
    "                K[i,j] = self.kernel(X[i], X[j])\n",
    "\n",
    "        P = cvxopt.matrix(np.outer(y,y) * K)\n",
    "        q = cvxopt.matrix(np.ones(n_samples) * -1)\n",
    "        A = cvxopt.matrix(y, (1,n_samples))\n",
    "        b = cvxopt.matrix(0.0)\n",
    "\n",
    "        if self.C is None:\n",
    "            G = cvxopt.matrix(np.diag(np.ones(n_samples) * -1))\n",
    "            h = cvxopt.matrix(np.zeros(n_samples))\n",
    "        else:\n",
    "            tmp1 = np.diag(np.ones(n_samples) * -1)\n",
    "            tmp2 = np.identity(n_samples)\n",
    "            G = cvxopt.matrix(np.vstack((tmp1, tmp2)))\n",
    "            tmp1 = np.zeros(n_samples)\n",
    "            tmp2 = np.ones(n_samples) * self.C\n",
    "            h = cvxopt.matrix(np.hstack((tmp1, tmp2)))\n",
    "\n",
    "        # solve QP problem\n",
    "        solution = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "\n",
    "        # Lagrange multipliers\n",
    "        a = np.ravel(solution['x'])\n",
    "\n",
    "        # Support vectors have non zero lagrange multipliers\n",
    "        sv = a > 1e-5\n",
    "        ind = np.arange(len(a))[sv]\n",
    "        self.a = a[sv]\n",
    "        self.sv = X[sv]\n",
    "        self.sv_y = y[sv]\n",
    "        print(\"%d support vectors out of %d points\" % (len(self.a), n_samples))\n",
    "\n",
    "        # Intercept\n",
    "        self.b = 0\n",
    "        for n in range(len(self.a)):\n",
    "            self.b += self.sv_y[n]\n",
    "            self.b -= np.sum(self.a * self.sv_y * K[ind[n],sv])\n",
    "        self.b /= len(self.a)\n",
    "\n",
    "        # Weight vector\n",
    "        if self.kernel == linear_kernel:\n",
    "            self.w = np.zeros(n_features)\n",
    "            for n in range(len(self.a)):\n",
    "                self.w += self.a[n] * self.sv_y[n] * self.sv[n]\n",
    "        else:\n",
    "            self.w = None\n",
    "\n",
    "    def project(self, X):\n",
    "        if self.w is not None:\n",
    "            return np.dot(X, self.w) + self.b\n",
    "        else:\n",
    "            y_predict = np.zeros(len(X))\n",
    "            for i in range(len(X)):\n",
    "                s = 0\n",
    "                for a, sv_y, sv in zip(self.a, self.sv_y, self.sv):\n",
    "                    s += a * sv_y * self.kernel(X[i], sv)\n",
    "                y_predict[i] = s\n",
    "            return y_predict + self.b\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.sign(self.project(X))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import pylab as pl\n",
    "\n",
    "    def gen_lin_separable_data():\n",
    "        # generate training data in the 2-d case\n",
    "        mean1 = np.array([0, 2])\n",
    "        mean2 = np.array([2, 0])\n",
    "        cov = np.array([[0.8, 0.6], [0.6, 0.8]])\n",
    "        X1 = np.random.multivariate_normal(mean1, cov, 100)\n",
    "        y1 = np.ones(len(X1))\n",
    "        X2 = np.random.multivariate_normal(mean2, cov, 100)\n",
    "        y2 = np.ones(len(X2)) * -1\n",
    "        return X1, y1, X2, y2\n",
    "\n",
    "    def gen_non_lin_separable_data():\n",
    "        mean1 = [-1, 2]\n",
    "        mean2 = [1, -1]\n",
    "        mean3 = [4, -4]\n",
    "        mean4 = [-4, 4]\n",
    "        cov = [[1.0,0.8], [0.8, 1.0]]\n",
    "        X1 = np.random.multivariate_normal(mean1, cov, 50)\n",
    "        X1 = np.vstack((X1, np.random.multivariate_normal(mean3, cov, 50)))\n",
    "        y1 = np.ones(len(X1))\n",
    "        X2 = np.random.multivariate_normal(mean2, cov, 50)\n",
    "        X2 = np.vstack((X2, np.random.multivariate_normal(mean4, cov, 50)))\n",
    "        y2 = np.ones(len(X2)) * -1\n",
    "        return X1, y1, X2, y2\n",
    "\n",
    "    def gen_lin_separable_overlap_data():\n",
    "        # generate training data in the 2-d case\n",
    "        mean1 = np.array([0, 2])\n",
    "        mean2 = np.array([2, 0])\n",
    "        cov = np.array([[1.5, 1.0], [1.0, 1.5]])\n",
    "        X1 = np.random.multivariate_normal(mean1, cov, 100)\n",
    "        y1 = np.ones(len(X1))\n",
    "        X2 = np.random.multivariate_normal(mean2, cov, 100)\n",
    "        y2 = np.ones(len(X2)) * -1\n",
    "        return X1, y1, X2, y2\n",
    "\n",
    "    def split_train(X1, y1, X2, y2):\n",
    "        X1_train = X1[:90]\n",
    "        y1_train = y1[:90]\n",
    "        X2_train = X2[:90]\n",
    "        y2_train = y2[:90]\n",
    "        X_train = np.vstack((X1_train, X2_train))\n",
    "        y_train = np.hstack((y1_train, y2_train))\n",
    "        return X_train, y_train\n",
    "\n",
    "    def split_test(X1, y1, X2, y2):\n",
    "        X1_test = X1[90:]\n",
    "        y1_test = y1[90:]\n",
    "        X2_test = X2[90:]\n",
    "        y2_test = y2[90:]\n",
    "        X_test = np.vstack((X1_test, X2_test))\n",
    "        y_test = np.hstack((y1_test, y2_test))\n",
    "        return X_test, y_test\n",
    "\n",
    "    def plot_margin(X1_train, X2_train, clf):\n",
    "        def f(x, w, b, c=0):\n",
    "            # given x, return y such that [x,y] in on the line\n",
    "            # w.x + b = c\n",
    "            return (-w[0] * x - b + c) / w[1]\n",
    "\n",
    "        pl.plot(X1_train[:,0], X1_train[:,1], \"ro\")\n",
    "        pl.plot(X2_train[:,0], X2_train[:,1], \"bo\")\n",
    "        pl.scatter(clf.sv[:,0], clf.sv[:,1], s=100, c=\"g\")\n",
    "\n",
    "        # w.x + b = 0\n",
    "        a0 = -4; a1 = f(a0, clf.w, clf.b)\n",
    "        b0 = 4; b1 = f(b0, clf.w, clf.b)\n",
    "        pl.plot([a0,b0], [a1,b1], \"k\")\n",
    "\n",
    "        # w.x + b = 1\n",
    "        a0 = -4; a1 = f(a0, clf.w, clf.b, 1)\n",
    "        b0 = 4; b1 = f(b0, clf.w, clf.b, 1)\n",
    "        pl.plot([a0,b0], [a1,b1], \"k--\")\n",
    "\n",
    "        # w.x + b = -1\n",
    "        a0 = -4; a1 = f(a0, clf.w, clf.b, -1)\n",
    "        b0 = 4; b1 = f(b0, clf.w, clf.b, -1)\n",
    "        pl.plot([a0,b0], [a1,b1], \"k--\")\n",
    "\n",
    "        pl.axis(\"tight\")\n",
    "        pl.show()\n",
    "\n",
    "    def plot_contour(X1_train, X2_train, clf):\n",
    "        pl.plot(X1_train[:,0], X1_train[:,1], \"ro\")\n",
    "        pl.plot(X2_train[:,0], X2_train[:,1], \"bo\")\n",
    "        pl.scatter(clf.sv[:,0], clf.sv[:,1], s=100, c=\"g\")\n",
    "\n",
    "        X1, X2 = np.meshgrid(np.linspace(-6,6,50), np.linspace(-6,6,50))\n",
    "        X = np.array([[x1, x2] for x1, x2 in zip(np.ravel(X1), np.ravel(X2))])\n",
    "        Z = clf.project(X).reshape(X1.shape)\n",
    "        pl.contour(X1, X2, Z, [0.0], colors='k', linewidths=1, origin='lower')\n",
    "        pl.contour(X1, X2, Z + 1, [0.0], colors='grey', linewidths=1, origin='lower')\n",
    "        pl.contour(X1, X2, Z - 1, [0.0], colors='grey', linewidths=1, origin='lower')\n",
    "\n",
    "        pl.axis(\"tight\")\n",
    "        pl.show()\n",
    "\n",
    "    def test_linear():\n",
    "        X1, y1, X2, y2 = gen_lin_separable_data()\n",
    "        X_train, y_train = split_train(X1, y1, X2, y2)\n",
    "        X_test, y_test = split_test(X1, y1, X2, y2)\n",
    "\n",
    "        clf = SVM()\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        y_predict = clf.predict(X_test)\n",
    "        correct = np.sum(y_predict == y_test)\n",
    "        print(\"%d out of %d predictions correct\" % (correct, len(y_predict)))\n",
    "\n",
    "        plot_margin(X_train[y_train==1], X_train[y_train==-1], clf)\n",
    "\n",
    "    def test_non_linear():\n",
    "        X1, y1, X2, y2 = gen_non_lin_separable_data()\n",
    "        X_train, y_train = split_train(X1, y1, X2, y2)\n",
    "        X_test, y_test = split_test(X1, y1, X2, y2)\n",
    "\n",
    "        clf = SVM(polynomial_kernel)\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        y_predict = clf.predict(X_test)\n",
    "        correct = np.sum(y_predict == y_test)\n",
    "        print(\"%d out of %d predictions correct\" % (correct, len(y_predict)))\n",
    "\n",
    "        plot_contour(X_train[y_train==1], X_train[y_train==-1], clf)\n",
    "\n",
    "    def test_soft():\n",
    "        X1, y1, X2, y2 = gen_lin_separable_overlap_data()\n",
    "        X_train, y_train = split_train(X1, y1, X2, y2)\n",
    "        X_test, y_test = split_test(X1, y1, X2, y2)\n",
    "\n",
    "        clf = SVM(C=1000.1)\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        y_predict = clf.predict(X_test)\n",
    "        correct = np.sum(y_predict == y_test)\n",
    "        print(\"%d out of %d predictions correct\" % (correct, len(y_predict)))\n",
    "\n",
    "        plot_contour(X_train[y_train==1], X_train[y_train==-1], clf)\n",
    "\n",
    "        \n",
    "    #test_linear()\n",
    "    #test_non_linear()\n",
    "    test_soft()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
